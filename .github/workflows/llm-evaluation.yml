name: 🤖 LLM-as-a-Judge Evaluation

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:  # Allow manual trigger
    inputs:
      test_scope:
        description: 'Test scope'
        required: true
        default: 'single'
        type: choice
        options:
        - single
        - three_cases
        - full

jobs:
  llm-evaluation:
    runs-on: ubuntu-latest
    timeout-minutes: 15  # Prevent runaway costs

    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      JUDGE_MODEL: gpt-5

    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v4

    - name: 🐍 Set up Python 3.11
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: 📦 Install dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt

    - name: 🔍 Verify environment
      run: |
        python --version
        python -c "import deepeval; print('✅ DeepEval:', deepeval.__version__)"
        python -c "import openai; print('✅ OpenAI client ready')"
        python -c "import anthropic; print('✅ Anthropic client ready')"

    - name: 🧪 Run LLM Evaluation (Single Test)
      if: github.event.inputs.test_scope != 'full' && github.event.inputs.test_scope != 'three_cases'
      run: |
        echo "🚀 Running single test case evaluation..."
        python -m pytest test_llm_evaluation.py \
          -k "test_case0 and test_correctness and claude" \
          --junitxml=test-results/junit-single.xml \
          --html=test-results/report-single.html \
          --self-contained-html \
          -v --tb=short

    - name: 🧪 Run LLM Evaluation (Three Cases)
      if: github.event.inputs.test_scope == 'three_cases'
      run: |
        echo "🚀 Running three test cases evaluation..."
        python -m pytest test_llm_evaluation.py \
          -k "(test_case0 or test_case1 or test_case2) and test_correctness" \
          --junitxml=test-results/junit-three.xml \
          --html=test-results/report-three.html \
          --self-contained-html \
          -v --tb=short --maxfail=3

    - name: 🧪 Run LLM Evaluation (Full Suite)
      if: github.event.inputs.test_scope == 'full'
      run: |
        echo "🚀 Running full evaluation suite..."
        echo "⚠️  This will cost approximately $10-20 USD"
        python -m pytest test_llm_evaluation.py \
          --junitxml=test-results/junit-full.xml \
          --html=test-results/report-full.html \
          --self-contained-html \
          -v --tb=short --maxfail=10

    - name: 📊 Parse Test Results
      if: always()
      run: |
        echo "## 📊 Test Results Summary" >> $GITHUB_STEP_SUMMARY

        if [ -f test-results/junit-single.xml ]; then
          JUNIT_FILE="test-results/junit-single.xml"
        elif [ -f test-results/junit-three.xml ]; then
          JUNIT_FILE="test-results/junit-three.xml"
        elif [ -f test-results/junit-full.xml ]; then
          JUNIT_FILE="test-results/junit-full.xml"
        fi

        if [ -f "$JUNIT_FILE" ]; then
          # Extract key metrics from JUnit XML
          TESTS=$(xmllint --xpath "//testsuite/@tests" "$JUNIT_FILE" | sed 's/tests="//;s/"//')
          FAILURES=$(xmllint --xpath "//testsuite/@failures" "$JUNIT_FILE" | sed 's/failures="//;s/"//')
          TIME=$(xmllint --xpath "//testsuite/@time" "$JUNIT_FILE" | sed 's/time="//;s/"//')

          echo "- **Tests Run**: $TESTS" >> $GITHUB_STEP_SUMMARY
          echo "- **Failures**: $FAILURES" >> $GITHUB_STEP_SUMMARY
          echo "- **Duration**: ${TIME}s" >> $GITHUB_STEP_SUMMARY

          # Extract costs and scores
          TOTAL_COST=$(xmllint --xpath "sum(//property[@name='api_cost_usd']/@value)" "$JUNIT_FILE" 2>/dev/null | sed 's/\$//g' || echo "N/A")
          echo "- **Total API Cost**: \$${TOTAL_COST}" >> $GITHUB_STEP_SUMMARY

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### 🎯 Model Performance" >> $GITHUB_STEP_SUMMARY
          echo "| Test | Model | Score | Cost | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|------|-------|--------|------|--------|" >> $GITHUB_STEP_SUMMARY

          # Parse individual test results (simplified)
          xmllint --xpath "//testcase" "$JUNIT_FILE" | while IFS= read -r testcase; do
            if [[ $testcase == *"name="* ]]; then
              echo "| Test case | Model | Score | Cost | ✅ |" >> $GITHUB_STEP_SUMMARY
            fi
          done 2>/dev/null || echo "| Results | Available | In | Reports | ✅ |" >> $GITHUB_STEP_SUMMARY
        fi

    - name: 📤 Upload Test Reports
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: llm-evaluation-reports
        path: |
          test-results/*.xml
          test-results/*.html
        retention-days: 30

    - name: 📋 Publish Test Results
      if: always()
      uses: dorny/test-reporter@v1
      with:
        name: 🤖 LLM Evaluation Results
        path: 'test-results/*.xml'
        reporter: java-junit
        fail-on-error: false

    - name: 💬 Comment on PR
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');
          const path = require('path');

          // Find JUnit XML file
          const testResults = fs.readdirSync('test-results').find(f => f.endsWith('.xml'));

          if (!testResults) {
            console.log('No test results found');
            return;
          }

          const xmlContent = fs.readFileSync(`test-results/${testResults}`, 'utf8');

          // Extract basic metrics (simplified parsing)
          const testsMatch = xmlContent.match(/tests="(\d+)"/);
          const failuresMatch = xmlContent.match(/failures="(\d+)"/);
          const timeMatch = xmlContent.match(/time="([\d.]+)"/);

          const tests = testsMatch ? testsMatch[1] : 'N/A';
          const failures = failuresMatch ? failuresMatch[1] : 'N/A';
          const time = timeMatch ? parseFloat(timeMatch[1]).toFixed(1) : 'N/A';

          const status = failures === '0' ? '✅ PASSED' : '❌ FAILED';
          const emoji = failures === '0' ? '🎉' : '🚨';

          const comment = `${emoji} **LLM-as-a-Judge Evaluation Results**

          | Metric | Value |
          |--------|-------|
          | Status | ${status} |
          | Tests Run | ${tests} |
          | Failures | ${failures} |
          | Duration | ${time}s |

          📊 **Detailed Reports**: Check the uploaded artifacts for complete HTML and JUnit XML reports.

          🤖 Evaluated with **GPT-5 as judge** using **DeepEval + pytest**.`;

          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

    - name: 🚨 Fail on Test Failures
      if: always()
      run: |
        if [ -f test-results/*.xml ]; then
          FAILURES=$(xmllint --xpath "//testsuite/@failures" test-results/*.xml | sed 's/failures="//;s/"//')
          if [ "$FAILURES" != "0" ]; then
            echo "❌ Tests failed with $FAILURES failures"
            exit 1
          fi
          echo "✅ All tests passed!"
        fi