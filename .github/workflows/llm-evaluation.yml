name: 🤖 LLM-as-a-Judge Evaluation

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:  # Allow manual trigger
    inputs:
      test_scope:
        description: 'Test scope'
        required: true
        default: 'single'
        type: choice
        options:
        - single
        - three_cases
        - full

jobs:
  llm-evaluation:
    runs-on: ubuntu-latest
    timeout-minutes: 15  # Prevent runaway costs

    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      JUDGE_MODEL: gpt-5

    steps:
    - name: 📥 Checkout code
      uses: actions/checkout@v5

    - name: 🐍 Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: 📦 Install dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt

    - name: 🔍 Verify environment
      run: |
        python --version
        python -c "import deepeval; print('✅ DeepEval:', deepeval.__version__)"
        python -c "import openai; print('✅ OpenAI client ready')"
        python -c "import anthropic; print('✅ Anthropic client ready')"

    - name: 🧪 Run LLM Evaluation (Single Test)
      if: github.event.inputs.test_scope != 'full' && github.event.inputs.test_scope != 'three_cases'
      run: |
        echo "🚀 Running single test case evaluation..."
        python -m pytest test_llm_evaluation.py \
          -k "test_case0 and test_correctness and claude" \
          --junitxml=test-results/junit.xml \
          --html=test-results/report.html \
          --self-contained-html \
          -v --tb=short

    - name: 🧪 Run LLM Evaluation (Three Cases)
      if: github.event.inputs.test_scope == 'three_cases'
      run: |
        echo "🚀 Running three test cases evaluation..."
        python -m pytest test_llm_evaluation.py \
          -k "(test_case0 or test_case1 or test_case2) and test_correctness" \
          --junitxml=test-results/junit.xml \
          --html=test-results/report.html \
          --self-contained-html \
          -v --tb=short --maxfail=3

    - name: 🧪 Run LLM Evaluation (Full Suite)
      if: github.event.inputs.test_scope == 'full'
      run: |
        echo "🚀 Running full evaluation suite..."
        echo "⚠️  This will cost approximately $10-20 USD"
        python -m pytest test_llm_evaluation.py \
          --junitxml=test-results/junit.xml \
          --html=test-results/report.html \
          --self-contained-html \
          -v --tb=short --maxfail=10

    - name: 📊 Simple Results Summary
      if: always()
      run: |
        echo "## 🤖 LLM-as-a-Judge Evaluation Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "📋 **Test Scope**: $(echo ${{ github.event.inputs.test_scope || 'single' }} | tr '[:lower:]' '[:upper:]')" >> $GITHUB_STEP_SUMMARY
        echo "🤖 **Judge Model**: GPT-5" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "📊 **Detailed results available in the test reports below**" >> $GITHUB_STEP_SUMMARY

    - name: 📤 Upload Test Reports
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: llm-evaluation-reports-${{ github.event.inputs.test_scope || 'single' }}
        path: test-results/
        retention-days: 30

    - name: 📋 Publish Test Results
      if: always()
      uses: dorny/test-reporter@v2
      with:
        name: 🤖 LLM Evaluation Results
        path: 'test-results/junit.xml'
        reporter: java-junit
        fail-on-error: false

    - name: 📈 Enhanced JUnit Report
      if: always()
      uses: EnricoMi/publish-unit-test-result-action@v2
      with:
        files: test-results/junit.xml
        check_name: "LLM Evaluation Detailed Results"

    - name: 🚨 Fail on Test Failures
      if: always()
      run: |
        if [ -f test-results/junit.xml ]; then
          FAILURES=$(xmllint --xpath "//testsuite/@failures" test-results/junit.xml | sed 's/failures="//;s/"//')
          if [ "$FAILURES" != "0" ]; then
            echo "❌ Tests failed with $FAILURES failures"
            exit 1
          fi
          echo "✅ All tests passed!"
        else
          echo "⚠️  No test results found"
          exit 1
        fi