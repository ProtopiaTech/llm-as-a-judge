name: ğŸ¤– LLM-as-a-Judge Evaluation

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:  # Allow manual trigger
    inputs:
      test_scope:
        description: 'Test scope'
        required: true
        default: 'single'
        type: choice
        options:
        - single
        - three_cases
        - full

jobs:
  llm-evaluation:
    runs-on: ubuntu-latest
    timeout-minutes: 15  # Prevent runaway costs

    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      JUDGE_MODEL: gpt-5

    steps:
    - name: ğŸ“¥ Checkout code
      uses: actions/checkout@v5

    - name: ğŸ Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: ğŸ“¦ Install dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt

    - name: ğŸ” Verify environment
      run: |
        python --version
        python -c "import deepeval; print('âœ… DeepEval:', deepeval.__version__)"
        python -c "import openai; print('âœ… OpenAI client ready')"
        python -c "import anthropic; print('âœ… Anthropic client ready')"

    - name: ğŸ§ª Run LLM Evaluation (Single Test)
      if: github.event.inputs.test_scope != 'full' && github.event.inputs.test_scope != 'three_cases'
      run: |
        echo "ğŸš€ Running single test case evaluation..."
        python -m pytest test_llm_evaluation.py \
          -k "test_case0 and test_correctness and claude" \
          --junitxml=test-results/junit.xml \
          --html=test-results/report.html \
          --self-contained-html \
          -v --tb=short

    - name: ğŸ§ª Run LLM Evaluation (Three Cases)
      if: github.event.inputs.test_scope == 'three_cases'
      run: |
        echo "ğŸš€ Running three test cases evaluation..."
        python -m pytest test_llm_evaluation.py \
          -k "(test_case0 or test_case1 or test_case2) and test_correctness" \
          --junitxml=test-results/junit.xml \
          --html=test-results/report.html \
          --self-contained-html \
          -v --tb=short --maxfail=3

    - name: ğŸ§ª Run LLM Evaluation (Full Suite)
      if: github.event.inputs.test_scope == 'full'
      run: |
        echo "ğŸš€ Running full evaluation suite..."
        echo "âš ï¸  This will cost approximately $10-20 USD"
        python -m pytest test_llm_evaluation.py \
          --junitxml=test-results/junit.xml \
          --html=test-results/report.html \
          --self-contained-html \
          -v --tb=short --maxfail=10

    - name: ğŸ“Š Simple Results Summary
      if: always()
      run: |
        echo "## ğŸ¤– LLM-as-a-Judge Evaluation Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "ğŸ“‹ **Test Scope**: $(echo ${{ github.event.inputs.test_scope || 'single' }} | tr '[:lower:]' '[:upper:]')" >> $GITHUB_STEP_SUMMARY
        echo "ğŸ¤– **Judge Model**: GPT-5" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "ğŸ“Š **Detailed results available in the test reports below**" >> $GITHUB_STEP_SUMMARY

    - name: ğŸ“¤ Upload Test Reports
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: llm-evaluation-reports-${{ github.event.inputs.test_scope || 'single' }}
        path: test-results/
        retention-days: 30

    - name: ğŸ“‹ Publish Test Results
      if: always()
      uses: dorny/test-reporter@v2
      with:
        name: ğŸ¤– LLM Evaluation Results
        path: 'test-results/junit.xml'
        reporter: java-junit
        fail-on-error: false

    - name: ğŸ“ˆ Enhanced JUnit Report
      if: always()
      uses: EnricoMi/publish-unit-test-result-action@v2
      with:
        files: test-results/junit.xml
        check_name: "LLM Evaluation Detailed Results"

    - name: ğŸ’ Rich Results Parser
      if: always()
      run: |
        python3 -c "
import xml.etree.ElementTree as ET
import os
from collections import defaultdict

try:
    tree = ET.parse('test-results/junit.xml')
    root = tree.getroot()

    # Extract basic metrics
    testsuite = root.find('.//testsuite')
    total_tests = testsuite.get('tests', '0')
    failures = testsuite.get('failures', '0')
    time = float(testsuite.get('time', '0'))

    status = 'âœ… PASSED' if failures == '0' else 'âŒ FAILED'
    scope = os.environ.get('TEST_SCOPE', 'single')

    # Parse all test cases and extract custom properties
    test_results = []
    total_cost = 0.0
    model_stats = defaultdict(lambda: {'tests': 0, 'cost': 0.0, 'scores': []})

    for testcase in root.findall('.//testcase'):
        test_name = testcase.get('name', 'Unknown')

        # Extract properties
        props = {}
        for prop in testcase.findall('.//property'):
            name = prop.get('name')
            value = prop.get('value')
            if name and value:
                props[name] = value

        # Parse cost
        cost_str = props.get('api_cost_usd', '\$0.00')
        try:
            cost = float(cost_str.replace('\$', ''))
            total_cost += cost
        except:
            cost = 0.0

        # Determine test type and extract relevant metrics
        if 'correctness' in test_name:
            metric_type = 'Correctness'
            score = props.get('correctness_score', 'N/A')
            threshold = props.get('correctness_threshold', 'N/A')
            passed = props.get('correctness_passed', 'False') == 'True'
            reason = props.get('correctness_reason', 'N/A')
        elif 'style' in test_name:
            metric_type = 'Style'
            score = props.get('style_score', 'N/A')
            threshold = props.get('style_threshold', 'N/A')
            passed = props.get('style_passed', 'False') == 'True'
            reason = props.get('style_reason', 'N/A')
        else:
            metric_type = 'Unknown'
            score = 'N/A'
            threshold = 'N/A'
            passed = testcase.find('failure') is None
            reason = 'N/A'

        # Extract model name
        model = props.get('model', 'Unknown')
        model_short = model.split('-')[0] if model != 'Unknown' else 'Unknown'

        # Update model stats
        if score != 'N/A':
            try:
                model_stats[model_short]['scores'].append(float(score))
            except:
                pass
        model_stats[model_short]['tests'] += 1
        model_stats[model_short]['cost'] += cost

        # Format for display
        test_short = test_name.split('[')[0] if '[' in test_name else test_name
        test_short = test_short.replace('test_', '').title()
        reason_short = reason[:40] + '...' if len(reason) > 40 else reason
        status_icon = 'âœ…' if passed else 'âŒ'

        test_results.append({
            'test': test_short,
            'model': model_short,
            'metric': metric_type,
            'score': score,
            'threshold': threshold,
            'status': status_icon,
            'cost': f'\${cost:.4f}',
            'reason': reason_short
        })

    # Generate summary
    summary = f'''## ğŸ¤– LLM-as-a-Judge Evaluation Results

### ğŸ“ˆ Overview
| Metric | Value |
|--------|-------|
| **Status** | {status} |
| **Scope** | {scope.upper()} |
| **Tests Run** | {total_tests} |
| **Failures** | {failures} |
| **Duration** | {time:.1f}s |
| **Judge Model** | GPT-5 |
| **Total API Cost** | \${total_cost:.4f} |

### ğŸ¯ Detailed Results
| Test | Model | Metric | Score | Threshold | Status | Cost | Reason |
|------|-------|--------|--------|-----------|--------|------|--------|'''

    for result in test_results:
        summary += f'\n| {result[\"test\"]} | {result[\"model\"]} | {result[\"metric\"]} | {result[\"score\"]} | {result[\"threshold\"]} | {result[\"status\"]} | {result[\"cost\"]} | {result[\"reason\"]} |'

    # Model performance breakdown
    if model_stats:
        summary += '\n\n### ğŸ† Model Performance\n| Model | Tests | Avg Score | Total Cost |\n|-------|-------|-----------|------------|'

        for model, stats in model_stats.items():
            if stats['scores']:
                avg_score = sum(stats['scores']) / len(stats['scores'])
                avg_score_str = f'{avg_score:.2f}'
            else:
                avg_score_str = 'N/A'

            summary += f'\n| {model} | {stats[\"tests\"]} | {avg_score_str} | \${stats[\"cost\"]:.4f} |'

    # Write to GitHub Step Summary
    with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as f:
        f.write(summary)

    print('âœ… Rich results summary generated successfully!')

except Exception as e:
    print(f'âŒ Error generating rich summary: {str(e)}')
    # Fallback basic summary
    summary = f'''## ğŸ¤– LLM-as-a-Judge Evaluation Results

âŒ Could not parse detailed results: {str(e)}

ğŸ“Š Check the standard JUnit reports below for basic test information.'''

    with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as f:
        f.write(summary)
"

