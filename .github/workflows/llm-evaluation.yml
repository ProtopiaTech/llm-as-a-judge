name: ü§ñ LLM-as-a-Judge Evaluation

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:  # Allow manual trigger
    inputs:
      test_scope:
        description: 'Test scope'
        required: true
        default: 'single'
        type: choice
        options:
        - single
        - three_cases
        - full

jobs:
  llm-evaluation:
    runs-on: ubuntu-latest
    timeout-minutes: 15  # Prevent runaway costs

    env:
      OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
      JUDGE_MODEL: gpt-5

    steps:
    - name: üì• Checkout code
      uses: actions/checkout@v5

    - name: üêç Set up Python 3.11
      uses: actions/setup-python@v6
      with:
        python-version: '3.11'
        cache: 'pip'

    - name: üì¶ Install dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt

    - name: üîç Verify environment
      run: |
        python --version
        python -c "import deepeval; print('‚úÖ DeepEval:', deepeval.__version__)"
        python -c "import openai; print('‚úÖ OpenAI client ready')"
        python -c "import anthropic; print('‚úÖ Anthropic client ready')"

    - name: üß™ Run LLM Evaluation (Single Test)
      if: github.event.inputs.test_scope != 'full' && github.event.inputs.test_scope != 'three_cases'
      run: |
        echo "üöÄ Running single test case evaluation..."
        python -m pytest test_llm_evaluation.py \
          -k "test_case0-0.3 and test_correctness" \
          --junitxml=test-results/junit.xml \
          --html=test-results/report.html \
          --self-contained-html \
          -v --tb=short

    - name: üß™ Run LLM Evaluation (Three Cases)
      if: github.event.inputs.test_scope == 'three_cases'
      run: |
        echo "üöÄ Running three test cases evaluation..."
        python -m pytest test_llm_evaluation.py \
          -k "(test_case0-0.3 or test_case1-0.3 or test_case2-0.3) and test_correctness" \
          --junitxml=test-results/junit.xml \
          --html=test-results/report.html \
          --self-contained-html \
          -v --tb=short

    - name: üß™ Run LLM Evaluation (Full Suite)
      if: github.event.inputs.test_scope == 'full'
      run: |
        echo "üöÄ Running full evaluation suite..."
        echo "‚ö†Ô∏è  This will cost approximately $10-20 USD"
        python -m pytest test_llm_evaluation.py \
          --junitxml=test-results/junit.xml \
          --html=test-results/report.html \
          --self-contained-html \
          -v --tb=short

    - name: üìä Simple Results Summary
      if: always()
      run: |
        echo "## ü§ñ LLM-as-a-Judge Evaluation Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "üìã **Test Scope**: $(echo ${{ github.event.inputs.test_scope || 'single' }} | tr '[:lower:]' '[:upper:]')" >> $GITHUB_STEP_SUMMARY
        echo "ü§ñ **Judge Model**: GPT-5" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "üìä **Detailed results available in the test reports below**" >> $GITHUB_STEP_SUMMARY

    - name: üì§ Upload Test Reports
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: llm-evaluation-reports-${{ github.event.inputs.test_scope || 'single' }}
        path: test-results/
        retention-days: 30

    - name: üìã Publish Test Results
      if: always()
      uses: dorny/test-reporter@v2
      with:
        name: ü§ñ LLM Evaluation Results
        path: 'test-results/junit.xml'
        reporter: java-junit
        fail-on-error: false

    - name: üìà Enhanced JUnit Report
      if: always()
      uses: EnricoMi/publish-unit-test-result-action@v2
      with:
        files: test-results/junit.xml
        check_name: "LLM Evaluation Detailed Results"

    - name: üíé Rich Results Parser
      if: always()
      run: |
        cat > parse_results.py << 'EOF'
        import xml.etree.ElementTree as ET
        import os
        from collections import defaultdict

        try:
            tree = ET.parse('test-results/junit.xml')
            root = tree.getroot()

            # Extract basic metrics
            testsuite = root.find('.//testsuite')
            total_tests = testsuite.get('tests', '0')
            failures = testsuite.get('failures', '0')
            time = float(testsuite.get('time', '0'))

            status = '‚úÖ PASSED' if failures == '0' else '‚ùå FAILED'
            scope = os.environ.get('TEST_SCOPE', 'single')

            # Parse all test cases and extract custom properties
            test_results = []
            total_cost = 0.0
            model_stats = defaultdict(lambda: {'tests': 0, 'cost': 0.0, 'scores': []})

            for testcase in root.findall('.//testcase'):
                test_name = testcase.get('name', 'Unknown')

                # Extract properties
                props = {}
                for prop in testcase.findall('.//property'):
                    name = prop.get('name')
                    value = prop.get('value')
                    if name and value:
                        props[name] = value

                # Parse cost
                cost_str = props.get('api_cost_usd', '$0.00')
                try:
                    cost = float(cost_str.replace('$', ''))
                    total_cost += cost
                except:
                    cost = 0.0

                # Determine test type and extract relevant metrics
                if 'correctness' in test_name:
                    metric_type = 'Correctness'
                    score = props.get('correctness_score', 'N/A')
                    threshold = props.get('correctness_threshold', 'N/A')
                    passed = props.get('correctness_passed', 'False') == 'True'
                    reason = props.get('correctness_reason', 'N/A')
                elif 'style' in test_name:
                    metric_type = 'Style'
                    score = props.get('style_score', 'N/A')
                    threshold = props.get('style_threshold', 'N/A')
                    passed = props.get('style_passed', 'False') == 'True'
                    reason = props.get('style_reason', 'N/A')
                else:
                    metric_type = 'Unknown'
                    score = 'N/A'
                    threshold = 'N/A'
                    passed = testcase.find('failure') is None
                    reason = 'N/A'

                # Extract model name (use full original name)
                model = props.get('model', 'Unknown')
                model_short = model  # Use full model name

                # Update model stats
                if score != 'N/A':
                    try:
                        model_stats[model_short]['scores'].append(float(score))
                    except:
                        pass
                model_stats[model_short]['tests'] += 1
                model_stats[model_short]['cost'] += cost

                # Format for display
                test_short = test_name.split('[')[0] if '[' in test_name else test_name
                test_short = test_short.replace('test_', '').title()
                reason_short = reason[:40] + '...' if len(reason) > 40 else reason
                status_icon = '‚úÖ' if passed else '‚ùå'

                test_results.append({
                    'test': test_short,
                    'model': model_short,
                    'metric': metric_type,
                    'score': score,
                    'threshold': threshold,
                    'status': status_icon,
                    'cost': f'${cost:.4f}',
                    'reason': reason_short
                })

            # Generate summary
            summary = f"""## ü§ñ LLM-as-a-Judge Evaluation Results

        ### üìà Overview
        | Metric | Value |
        |--------|-------|
        | **Status** | {status} |
        | **Scope** | {scope.upper()} |
        | **Tests Run** | {total_tests} |
        | **Failures** | {failures} |
        | **Duration** | {time:.1f}s |
        | **Judge Model** | GPT-5 |
        | **Total API Cost** | ${total_cost:.4f} |

        ### üéØ Detailed Results
        | Test | Model | Metric | Score | Threshold | Status | Cost | Reason |
        |------|-------|--------|--------|-----------|--------|------|--------|"""

            for result in test_results:
                summary += f"""
        | {result["test"]} | {result["model"]} | {result["metric"]} | {result["score"]} | {result["threshold"]} | {result["status"]} | {result["cost"]} | {result["reason"]} |"""

            # Model performance breakdown
            if model_stats:
                summary += """

        ### üèÜ Model Performance
        | Model | Tests | Avg Score | Total Cost |
        |-------|-------|-----------|------------|"""

                for model, stats in model_stats.items():
                    if stats['scores']:
                        avg_score = sum(stats['scores']) / len(stats['scores'])
                        avg_score_str = f'{avg_score:.2f}'
                    else:
                        avg_score_str = 'N/A'

                    summary += f"""
        | {model} | {stats["tests"]} | {avg_score_str} | ${stats["cost"]:.4f} |"""

            # Write to GitHub Step Summary
            with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as f:
                f.write(summary)

            print('‚úÖ Rich results summary generated successfully!')

        except Exception as e:
            print(f'‚ùå Error generating rich summary: {str(e)}')
            # Fallback basic summary
            summary = f"""## ü§ñ LLM-as-a-Judge Evaluation Results

        ‚ùå Could not parse detailed results: {str(e)}

        üìä Check the standard JUnit reports below for basic test information."""

            with open(os.environ['GITHUB_STEP_SUMMARY'], 'a') as f:
                f.write(summary)
        EOF

        export TEST_SCOPE="${{ github.event.inputs.test_scope || 'single' }}"
        python3 parse_results.py

